From 2bc08fc92d59d8161254ca9cab477b575d35d926 Mon Sep 17 00:00:00 2001
From: Scott B <arglebargle@arglebargle.dev>
Date: Sun, 20 Feb 2022 16:42:33 -0800
Subject: [PATCH] btrfs autodefrag fixup and safe perf from -next
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Squashed commit of the following:

commit 706ad77549b43403effa92ed50287be79cc316ca
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri Feb 11 14:41:43 2022 +0800

    btrfs: defrag: make btrfs_defrag_file() to report accurate number of defragged sectors

    Previously rework btrfs_defrag_file() can only report the number of
    sectors from the first run of defrag_collect_targets().

    This number is not accurate as if holes are punched after the first
    defrag_collect_targets() call, we will not choose to defrag the holes.

    Originally this is to avoid passing @sectors_defragged to every involved
    functions.

    But now since we have btrfs_defrag_ctrl, there is no need to do such
    inaccurate accounting, just update btrfs_defrag_ctrl::sectors_defragged
    after a successful defrag_one_locked_target() call.

    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>

commit 8f313acbeecee49d2eb8ff518c84e981c0e23622
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri Feb 11 14:41:42 2022 +0800

    btrfs: defrag: use btrfs_defrag_ctrl to replace btrfs_ioctl_defrag_range_args for btrfs_defrag_file()

    This brings the following benefits:

    - No more strange range->start update to indicate last scanned bytenr
      We have btrfs_defrag_ctrl::last_scanned (exclusive) for it directly.

    - No more return value to indicate defragged sectors
      Now btrfs_defrag_file() will just return 0 if no error happened.
      And btrfs_defrag_ctrl::sectors_defragged will show that value.

    - Less parameters to carry around
      Now most defrag_* functions only need to fetch its policy parameters
      from btrfs_defrag_ctrl directly.

    Signed-off-by: Qu Wenruo <wqu@suse.com>

commit 34520c88647b10ff08fa333b7be314aa3e2e750d
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri Feb 11 14:41:41 2022 +0800

    btrfs: defrag: introduce btrfs_defrag_ctrl structure for later usage

    Currently btrfs_defrag_file() accepts not only
    btrfs_ioctl_defrag_range_args but also other parameters like @newer_than
    and @max_sectors_to_defrag for extra policies.

    Those extra values are hidden from defrag ioctl and even caused bugs in
    the past due to different behaviors based on those extra values.

    Here we introduce a new structure, btrfs_defrag_ctrl, to include:

    - all members in btrfs_ioctl_defrag_range_args

    - @max_sectors_to_defrag and @newer_than

    - Extra values which callers of btrfs_defrag_file() may care
      Like @sectors_defragged and @last_scanned.

    With the new structure, also introduce a new helper,
    btrfs_defrag_ioctl_args_to_ctrl() to:

    - Do extra sanity check on @compress and @flags

    - Do range alignment when possible

    - Set default values.

    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>

commit 448b2dc14374192edecad273275fc423385e4173
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri Feb 11 14:41:39 2022 +0800

    btrfs: defrag: allow defrag_one_cluster() to skip large extent which is not a target

    In the rework of btrfs_defrag_file(), we always call
    defrag_one_cluster() and increase the offset by cluster size, which is
    only 256K.

    But there are cases where we have a large extent (e.g. 128M) which
    doesn't need to be defragged at all.

    Before the refactor, we can directly skip the range, but now we have to
    scan that extent map again and again until the cluster moves after the
    non-target extent.

    Fix the problem by allow defrag_one_cluster() to increase
    btrfs_defrag_ctrl::last_scanned to the end of an extent, if and only if
    the last extent of the cluster is not a target.

    The test script looks like this:

    	mkfs.btrfs -f $dev > /dev/null

    	mount $dev $mnt

    	# As btrfs ioctl uses 32M as extent_threshold
    	xfs_io -f -c "pwrite 0 64M" $mnt/file1
    	sync
    	# Some fragemented range to defrag
    	xfs_io -s -c "pwrite 65548k 4k" \
    		  -c "pwrite 65544k 4k" \
    		  -c "pwrite 65540k 4k" \
    		  -c "pwrite 65536k 4k" \
    		  $mnt/file1
    	sync

    	echo "=== before ==="
    	xfs_io -c "fiemap -v" $mnt/file1
    	echo "=== after ==="
    	btrfs fi defrag $mnt/file1
    	sync
    	xfs_io -c "fiemap -v" $mnt/file1
    	umount $mnt

    With extra ftrace put into defrag_one_cluster(), before the patch it
    would result tons of loops:

    (As defrag_one_cluster() is inlined, the function name is its caller)

      btrfs-126062  [005] .....  4682.816026: btrfs_defrag_file: r/i=5/257 start=0 len=262144
      btrfs-126062  [005] .....  4682.816027: btrfs_defrag_file: r/i=5/257 start=262144 len=262144
      btrfs-126062  [005] .....  4682.816028: btrfs_defrag_file: r/i=5/257 start=524288 len=262144
      btrfs-126062  [005] .....  4682.816028: btrfs_defrag_file: r/i=5/257 start=786432 len=262144
      btrfs-126062  [005] .....  4682.816028: btrfs_defrag_file: r/i=5/257 start=1048576 len=262144
      ...
      btrfs-126062  [005] .....  4682.816043: btrfs_defrag_file: r/i=5/257 start=67108864 len=262144

    But with this patch there will be just one loop, then directly to the
    end of the extent:

      btrfs-130471  [014] .....  5434.029558: defrag_one_cluster: r/i=5/257 start=0 len=262144
      btrfs-130471  [014] .....  5434.029559: defrag_one_cluster: r/i=5/257 start=67108864 len=16384

    Cc: stable@vger.kernel.org # 5.16
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>

commit bb0c3d4f6814da192a0f75101f16dc8ce416026d
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri Feb 11 14:41:40 2022 +0800

    btrfs: uapi: introduce BTRFS_DEFRAG_RANGE_MASK for later sanity check

    And since we're here, replace the hardcoded bit flags (1, 2) with
    (1UL << 0) and (1UL << 1), respectively.

    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>

commit f16ef62642a47c7b884340f94f48c7a53f4d99e3
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri Jan 28 15:21:22 2022 +0800

    btrfs: defrag: remove an ambiguous condition for rejection

    From the very beginning of btrfs defrag, there is a check to reject
    extents which meet both conditions:

    - Physically adjacent

      We may want to defrag physically adjacent extents to reduce the number
      of extents or the size of subvolume tree.

    - Larger than 128K

      This may be there for compressed extents, but unfortunately 128K is
      exactly the max capacity for compressed extents.
      And the check is > 128K, thus it never rejects compressed extents.

      Furthermore, the compressed extent capacity bug is fixed by previous
      patch, there is no reason for that check anymore.

    The original check has a very small ranges to reject (the target extent
    size is > 128K, and default extent threshold is 256K), and for
    compressed extent it doesn't work at all.

    So it's better just to remove the rejection, and allow us to defrag
    physically adjacent extents.

    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 2428ba67520a80862df8338c92e8dfd7a7389239
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri Jan 28 15:21:21 2022 +0800

    btrfs: defrag: don't defrag extents which is already at its max capacity

    [BUG]
    For compressed extents, defrag ioctl will always try to defrag any
    compressed extents, wasting not only IO but also CPU time to
    compress/decompress:

       mkfs.btrfs -f $DEV
       mount -o compress $DEV $MNT
       xfs_io -f -c "pwrite -S 0xab 0 128K" $MNT/foobar
       sync
       xfs_io -f -c "pwrite -S 0xcd 128K 128K" $MNT/foobar
       sync
       echo "=== before ==="
       xfs_io -c "fiemap -v" $MNT/foobar
       btrfs filesystem defrag $MNT/foobar
       sync
       echo "=== after ==="
       xfs_io -c "fiemap -v" $MNT/foobar

    Then it shows the 2 128K extents just get CoW for no extra benefit, with
    extra IO/CPU spent:

        === before ===
        /mnt/btrfs/file1:
         EXT: FILE-OFFSET      BLOCK-RANGE      TOTAL FLAGS
           0: [0..255]:        26624..26879       256   0x8
           1: [256..511]:      26632..26887       256   0x9
        === after ===
        /mnt/btrfs/file1:
         EXT: FILE-OFFSET      BLOCK-RANGE      TOTAL FLAGS
           0: [0..255]:        26640..26895       256   0x8
           1: [256..511]:      26648..26903       256   0x9

    This affects not only v5.16 (after the defrag rework), but also v5.15
    (before the defrag rework).

    [CAUSE]
    >From the very beginning, btrfs defrag never checks if one extent is
    already at its max capacity (128K for compressed extents, 128M
    otherwise).

    And the default extent size threshold is 256K, which is already beyond
    the compressed extent max size.

    This means, by default btrfs defrag ioctl will mark all compressed
    extent which is not adjacent to a hole/preallocated range for defrag.

    [FIX]
    Introduce a helper to grab the maximum extent size, and then in
    defrag_collect_targets() and defrag_check_next_extent(), reject extents
    which are already at their max capacity.

    Reported-by: Filipe Manana <fdmanana@suse.com>
    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit b34776f51f2dcb31a6eb5fb3a7a2fd3e07535999
Author: Qu Wenruo <wqu@suse.com>
Date:   Fri Jan 28 15:21:20 2022 +0800

    btrfs: defrag: don't try to merge regular extents with preallocated extents

    [BUG]
    With older kernels (before v5.16), btrfs will defrag preallocated extents.
    While with newer kernels (v5.16 and newer) btrfs will not defrag
    preallocated extents, but it will defrag the extent just before the
    preallocated extent, even it's just a single sector.

    This can be exposed by the following small script:

    	mkfs.btrfs -f $dev > /dev/null

    	mount $dev $mnt
    	xfs_io -f -c "pwrite 0 4k" -c sync -c "falloc 4k 16K" $mnt/file
    	xfs_io -c "fiemap -v" $mnt/file
    	btrfs fi defrag $mnt/file
    	sync
    	xfs_io -c "fiemap -v" $mnt/file

    The output looks like this on older kernels:

    /mnt/btrfs/file:
     EXT: FILE-OFFSET      BLOCK-RANGE      TOTAL FLAGS
       0: [0..7]:          26624..26631         8   0x0
       1: [8..39]:         26632..26663        32 0x801
    /mnt/btrfs/file:
     EXT: FILE-OFFSET      BLOCK-RANGE      TOTAL FLAGS
       0: [0..39]:         26664..26703        40   0x1

    Which defrags the single sector along with the preallocated extent, and
    replace them with an regular extent into a new location (caused by data
    COW).
    This wastes most of the data IO just for the preallocated range.

    On the other hand, v5.16 is slightly better:

    /mnt/btrfs/file:
     EXT: FILE-OFFSET      BLOCK-RANGE      TOTAL FLAGS
       0: [0..7]:          26624..26631         8   0x0
       1: [8..39]:         26632..26663        32 0x801
    /mnt/btrfs/file:
     EXT: FILE-OFFSET      BLOCK-RANGE      TOTAL FLAGS
       0: [0..7]:          26664..26671         8   0x0
       1: [8..39]:         26632..26663        32 0x801

    The preallocated range is not defragged, but the sector before it still
    gets defragged, which has no need for it.

    [CAUSE]
    One of the function reused by the old and new behavior is
    defrag_check_next_extent(), it will determine if we should defrag
    current extent by checking the next one.

    It only checks if the next extent is a hole or inlined, but it doesn't
    check if it's preallocated.

    On the other hand, out of the function, both old and new kernel will
    reject preallocated extents.

    Such inconsistent behavior causes above behavior.

    [FIX]
    - Also check if next extent is preallocated
      If so, don't defrag current extent.

    - Add comments for each branch why we reject the extent

    This will reduce the IO caused by defrag ioctl and autodefrag.

    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 214928488f48f377bd913388bbaf0a0a776a76fe
Author: Sidong Yang <realwakka@gmail.com>
Date:   Sun Feb 6 12:52:48 2022 +0000

    btrfs: qgroup: remove duplicated check in adding qgroup relations

    Removes duplicated check when adding qgroup relations.
    btrfs_add_qgroup_relations function adds relations by calling
    add_relation_rb(). add_relation_rb() checks that member/parentid exists
    in current qgroup_tree. But it already checked before calling the
    function. It seems that we don't need to double check.

    Add new function __add_relation_rb() that adds relations with
    qgroup structures and makes old function use the new one. And it makes
    btrfs_add_qgroup_relation() function work without double checks by
    calling the new function.

    Signed-off-by: Sidong Yang <realwakka@gmail.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    [ add comments ]
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 2bf197b253b7e325c857645ccf857a32747ec580
Author: Dāvis Mosāns <davispuh@gmail.com>
Date:   Wed Feb 2 23:44:54 2022 +0200

    btrfs: add lzo workspace buffer length constants

    It makes it more readable for length checking and is be used repeatedly.

    Signed-off-by: Dāvis Mosāns <davispuh@gmail.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 61082b1ccdb30c9b2de058c410ca92454590398c
Author: Dāvis Mosāns <davispuh@gmail.com>
Date:   Wed Feb 2 23:44:55 2022 +0200

    btrfs: prevent copying too big compressed lzo segment

    Compressed length can be corrupted to be a lot larger than memory
    we have allocated for buffer.
    This will cause memcpy in copy_compressed_segment to write outside
    of allocated memory.

    This mostly results in stuck read syscall but sometimes when using
    btrfs send can get #GP

      kernel: general protection fault, probably for non-canonical address 0x841551d5c1000: 0000 [#1] PREEMPT SMP NOPTI
      kernel: CPU: 17 PID: 264 Comm: kworker/u256:7 Tainted: P           OE     5.17.0-rc2-1 #12
      kernel: Workqueue: btrfs-endio btrfs_work_helper [btrfs]
      kernel: RIP: 0010:lzo_decompress_bio (./include/linux/fortify-string.h:225 fs/btrfs/lzo.c:322 fs/btrfs/lzo.c:394) btrfs
      Code starting with the faulting instruction
      ===========================================
         0:*  48 8b 06                mov    (%rsi),%rax              <-- trapping instruction
         3:   48 8d 79 08             lea    0x8(%rcx),%rdi
         7:   48 83 e7 f8             and    $0xfffffffffffffff8,%rdi
         b:   48 89 01                mov    %rax,(%rcx)
         e:   44 89 f0                mov    %r14d,%eax
        11:   48 8b 54 06 f8          mov    -0x8(%rsi,%rax,1),%rdx
      kernel: RSP: 0018:ffffb110812efd50 EFLAGS: 00010212
      kernel: RAX: 0000000000001000 RBX: 000000009ca264c8 RCX: ffff98996e6d8ff8
      kernel: RDX: 0000000000000064 RSI: 000841551d5c1000 RDI: ffffffff9500435d
      kernel: RBP: ffff989a3be856c0 R08: 0000000000000000 R09: 0000000000000000
      kernel: R10: 0000000000000000 R11: 0000000000001000 R12: ffff98996e6d8000
      kernel: R13: 0000000000000008 R14: 0000000000001000 R15: 000841551d5c1000
      kernel: FS:  0000000000000000(0000) GS:ffff98a09d640000(0000) knlGS:0000000000000000
      kernel: CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
      kernel: CR2: 00001e9f984d9ea8 CR3: 000000014971a000 CR4: 00000000003506e0
      kernel: Call Trace:
      kernel:  <TASK>
      kernel: end_compressed_bio_read (fs/btrfs/compression.c:104 fs/btrfs/compression.c:1363 fs/btrfs/compression.c:323) btrfs
      kernel: end_workqueue_fn (fs/btrfs/disk-io.c:1923) btrfs
      kernel: btrfs_work_helper (fs/btrfs/async-thread.c:326) btrfs
      kernel: process_one_work (./arch/x86/include/asm/jump_label.h:27 ./include/linux/jump_label.h:212 ./include/trace/events/workqueue.h:108 kernel/workqueue.c:2312)
      kernel: worker_thread (./include/linux/list.h:292 kernel/workqueue.c:2455)
      kernel: ? process_one_work (kernel/workqueue.c:2397)
      kernel: kthread (kernel/kthread.c:377)
      kernel: ? kthread_complete_and_exit (kernel/kthread.c:332)
      kernel: ret_from_fork (arch/x86/entry/entry_64.S:301)
      kernel:  </TASK>

    CC: stable@vger.kernel.org # 4.9+
    Signed-off-by: Dāvis Mosāns <davispuh@gmail.com>
    Reviewed-by: David Sterba <dsterba@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 5ee929f88d095666e49e6cdec57777a4f92e24ba
Author: Qu Wenruo <wqu@suse.com>
Date:   Tue Feb 8 13:31:19 2022 +0800

    btrfs: populate extent_map::generation when reading from disk

    When btrfs_get_extent() tries to get some file extent from disk, it
    never populates extent_map::generation, leaving the value to be 0.

    On the other hand, for extent map generated by IO, it will get its
    generation properly set at finish_ordered_io()

     finish_ordered_io()
     |- unpin_extent_cache(gen = trans->transid)
        |- em->generation = gen;

    [CAUSE]
    Since extent_map::generation is mostly used by fsync code, and for fsync
    they only care about modified extents, which all have their
    em::generation > 0.

    Thus it's fine to not populate em read from disk for fsync.

    [CORNER CASE]
    However autodefrag also relies on em::generation to determine if one
    extent needs to be defragged.

    This unpopulated extent_map::generation can prevent the following
    autodefrag case from working:

    	mkfs.btrfs -f $dev
    	mount $dev $mnt -o autodefrag

    	# initial write to queue the inode for autodefrag
    	xfs_io -f -c "pwrite 0 4k" $mnt/file
    	sync

    	# Real fragmented write
    	xfs_io -f -s -c "pwrite -b 4096 0 32k" $mnt/file
    	sync
    	echo "=== before autodefrag ==="
    	xfs_io -c "fiemap -v" $mnt/file

    	# Drop cache to force em to be read from disk
    	echo 3 > /proc/sys/vm/drop_caches
    	mount -o remount,commit=1 $mnt
    	sleep 3
    	sync

    	echo "=== After autodefrag ==="
    	xfs_io -c "fiemap -v" $mnt/file
    	umount $mnt

    The result looks like this:

      === before autodefrag ===
      /mnt/btrfs/file:
       EXT: FILE-OFFSET      BLOCK-RANGE      TOTAL FLAGS
         0: [0..15]:         26672..26687        16   0x0
         1: [16..31]:        26656..26671        16   0x0
         2: [32..47]:        26640..26655        16   0x0
         3: [48..63]:        26624..26639        16   0x1
      === After autodefrag ===
      /mnt/btrfs/file:
       EXT: FILE-OFFSET      BLOCK-RANGE      TOTAL FLAGS
         0: [0..15]:         26672..26687        16   0x0
         1: [16..31]:        26656..26671        16   0x0
         2: [32..47]:        26640..26655        16   0x0
         3: [48..63]:        26624..26639        16   0x1

    This fragmented 32K will not be defragged by autodefrag.

    [FIX]
    To make things less weird, just populate extent_map::generation when
    reading file extents from disk.

    This would make above fragmented extents to be properly defragged:

      == before autodefrag ===
      /mnt/btrfs/file:
       EXT: FILE-OFFSET      BLOCK-RANGE      TOTAL FLAGS
         0: [0..15]:         26672..26687        16   0x0
         1: [16..31]:        26656..26671        16   0x0
         2: [32..47]:        26640..26655        16   0x0
         3: [48..63]:        26624..26639        16   0x1
      === After autodefrag ===
      /mnt/btrfs/file:
       EXT: FILE-OFFSET      BLOCK-RANGE      TOTAL FLAGS
         0: [0..63]:         26688..26751        64   0x1

    Reviewed-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: Qu Wenruo <wqu@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 8acdf473ad82d15c2d3e743ef31523fc178e7766
Author: Filipe Manana <fdmanana@suse.com>
Date:   Thu Feb 3 14:55:50 2022 +0000

    btrfs: prepare extents to be logged before locking a log tree path

    When we want to log an extent, in the fast fsync path, we obtain a path
    to the leaf that will hold the file extent item either through a deletion
    search, via btrfs_drop_extents(), or through an insertion search using
    btrfs_insert_empty_item(). After that we fill the file extent item's
    fields one by one directly on the leaf.

    Instead of doing that, we could prepare the file extent item before
    obtaining a btree path, and then copy the prepared extent item with a
    single operation once we get the path. This helps avoid some contention
    on the log tree, since we are holding write locks for longer than
    necessary, especially in the case where the path is obtained via
    btrfs_drop_extents() through a deletion search, which always keeps a
    write lock on the nodes at levels 1 and 2 (besides the leaf).

    This change does that, we prepare the file extent item that is going to
    be inserted before acquiring a path, and then copy it into a leaf using
    a single copy operation once we get a path.

    This change if part of a patchset that is comprised of the following
    patches:

      1/6 btrfs: remove unnecessary leaf free space checks when pushing items
      2/6 btrfs: avoid unnecessary COW of leaves when deleting items from a leaf
      3/6 btrfs: avoid unnecessary computation when deleting items from a leaf
      4/6 btrfs: remove constraint on number of visited leaves when replacing extents
      5/6 btrfs: remove useless path release in the fast fsync path
      6/6 btrfs: prepare extents to be logged before locking a log tree path

    The following test was run to measure the impact of the whole patchset:

      $ cat test.sh
      #!/bin/bash

      DEV=/dev/sdi
      MNT=/mnt/sdi
      MOUNT_OPTIONS="-o ssd"
      MKFS_OPTIONS="-R free-space-tree -O no-holes"

      NUM_JOBS=8
      FILE_SIZE=128M
      RUN_TIME=200

      cat <<EOF > /tmp/fio-job.ini
      [writers]
      rw=randwrite
      fsync=1
      fallocate=none
      group_reporting=1
      direct=0
      bssplit=4k/20:8k/20:16k/20:32k/10:64k/10:128k/5:256k/5:512k/5:1m/5
      ioengine=sync
      filesize=$FILE_SIZE
      runtime=$RUN_TIME
      time_based
      directory=$MNT
      numjobs=$NUM_JOBS
      thread
      EOF

      echo "performance" | \
          tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

      echo
      echo "Using config:"
      echo
      cat /tmp/fio-job.ini
      echo

      umount $MNT &> /dev/null
      mkfs.btrfs -f $MKFS_OPTIONS $DEV
      mount $MOUNT_OPTIONS $DEV $MNT

      fio /tmp/fio-job.ini

      umount $MNT

    The test ran inside a VM (8 cores, 32G of RAM) with the target disk
    mapping to a raw NVMe device, and using a non-debug kernel config
    (Debian's default config).

    Before the patchset:

    WRITE: bw=116MiB/s (122MB/s), 116MiB/s-116MiB/s (122MB/s-122MB/s), io=22.7GiB (24.4GB), run=200013-200013msec

    After the patchset:

    WRITE: bw=125MiB/s (131MB/s), 125MiB/s-125MiB/s (131MB/s-131MB/s), io=24.3GiB (26.1GB), run=200007-200007msec

    A 7.8% gain on throughput and +7.0% more IO done in the same period of
    time (200 seconds).

    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 9e21a3212cb0b4bd5f9097817336f96e8dfd503d
Author: Filipe Manana <fdmanana@suse.com>
Date:   Thu Feb 3 14:55:49 2022 +0000

    btrfs: remove useless path release in the fast fsync path

    There's no point in calling btrfs_release_path() after finishing the loop
    that logs the modified extents, since log_one_extent() returns with the
    path released. In case the list of extents is empty, the path is already
    released, so there's no need for that case as well.
    So just remove that unnecessary btrfs_release_path() call.

    This change if part of a patchset that is comprised of the following
    patches:

      1/6 btrfs: remove unnecessary leaf free space checks when pushing items
      2/6 btrfs: avoid unnecessary COW of leaves when deleting items from a leaf
      3/6 btrfs: avoid unnecessary computation when deleting items from a leaf
      4/6 btrfs: remove constraint on number of visited leaves when replacing extents
      5/6 btrfs: remove useless path release in the fast fsync path
      6/6 btrfs: prepare extents to be logged before locking a log tree path

    The last patch in the series has some performance test result in its
    changelog.

    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit be6de6a0ade5167e4422403557203b988f892e43
Author: Filipe Manana <fdmanana@suse.com>
Date:   Thu Feb 3 14:55:48 2022 +0000

    btrfs: remove constraint on number of visited leaves when replacing extents

    At btrfs_drop_extents(), we try to replace a range of file extent items
    with a new file extent in a single btree search, to avoid the need to do
    a search for deletion, followed by a path release and followed by yet
    another search for insertion.

    When I originally added that optimization, in commit 1acae57b161ef1
    ("Btrfs: faster file extent item replace operations"), I left a constraint
    to do the fast replace only if we visited a single leaf. That was because
    in the most common case we find all file extent items that need to be
    deleted (or trimmed) in a single leaf, however it can work for other
    common cases like when we need to delete a few file extent items located
    at the end of a leaf and a few more located at the beginning of the next
    leaf. The key for the new file extent item is greater than the key of
    any deleted or trimmed file extent item from previous leaves, so we are
    fine to use the last leaf that we found as long as we are holding a
    write lock on it - even if the new key ends up at slot 0, as if that's
    the case, the btree search has obtained a write lock on any upper nodes
    that need to have a key pointer updated.

    So removed the constraint that limits the optimization to the case where
    we visited only a single leaf.

    This change if part of a patchset that is comprised of the following
    patches:

      1/6 btrfs: remove unnecessary leaf free space checks when pushing items
      2/6 btrfs: avoid unnecessary COW of leaves when deleting items from a leaf
      3/6 btrfs: avoid unnecessary computation when deleting items from a leaf
      4/6 btrfs: remove constraint on number of visited leaves when replacing extents
      5/6 btrfs: remove useless path release in the fast fsync path
      6/6 btrfs: prepare extents to be logged before locking a log tree path

    The last patch in the series has some performance test result in its
    changelog.

    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 7f7eb0de92fcda44221079ae4797811ef7c32c40
Author: Filipe Manana <fdmanana@suse.com>
Date:   Thu Feb 3 14:55:47 2022 +0000

    btrfs: avoid unnecessary computation when deleting items from a leaf

    When deleting items from a leaf, we always compute the sum of the data
    sizes of the items that are going to be deleted. However we only use
    that sum when the last item to delete is behind the last item in the
    leaf. This unnecessarily wastes CPU time when we are deleting either
    the whole leaf or from some slot > 0 up to the last item in the leaf,
    and both of these cases are common (e.g. truncation operation, either
    as a result of truncate(2) or when logging inodes, deleting checksums
    after removing a large enough extent, etc).

    So compute only the sum of the data sizes if the last item to be
    deleted does not match the last item in the leaf.

    This change if part of a patchset that is comprised of the following
    patches:

      1/6 btrfs: remove unnecessary leaf free space checks when pushing items
      2/6 btrfs: avoid unnecessary COW of leaves when deleting items from a leaf
      3/6 btrfs: avoid unnecessary computation when deleting items from a leaf
      4/6 btrfs: remove constraint on number of visited leaves when replacing extents
      5/6 btrfs: remove useless path release in the fast fsync path
      6/6 btrfs: prepare extents to be logged before locking a log tree path

    The last patch in the series has some performance test result in its
    changelog.

    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 7856495147f67321110df817526efe0181ab53bc
Author: Filipe Manana <fdmanana@suse.com>
Date:   Thu Feb 3 14:55:46 2022 +0000

    btrfs: avoid unnecessary COW of leaves when deleting items from a leaf

    When we delete items from a leaf, if we end up with more than two thirds
    of unused leaf space, we try to delete the leaf by moving all its items
    into its left and right neighbour leaves. Sometimes that is not possible
    because there is not enough free space in the left and right leaves, and
    in that case we end up not deleting our leaf.

    The way we are doing this is not ideal and can be improved in the
    following ways:

    1) When we call push_leaf_left(), we pass a value of 1 byte to the data
       size parameter of push_leaf_left(). This is not realistic value because
       no item can have a size less than 25 bytes, which is the size of struct
       btrfs_item. This means that means that if the left leaf has not enough
       free space to push any item, we end up COWing it even if we end up not
       changing its content at all.

       COWing that leaf means allocating a new metadata extent, marking it
       dirty and doing more IO when committing a transaction or when syncing a
       log tree. For a log tree case, it's particularly more important to
       avoid the useless COW operation, as more IO can imply a higher latency
       for an fsync operation.

       So instead of passing 1 as the minimum data size for push_leaf_left(),
       pass the size of the first item in our leaf, as we don't want to COW
       the left leaf if we can't at least push the first item of our leaf;

    2) When we call push_leaf_right(), we also pass a value of 1 byte as the
       data size parameter of push_leaf_right(). Like the previous case, it
       will also result in COWing the right leaf even if we are not able to
       move any items into it, since there can't be any item with a size
       smaller than 25 bytes (the size of struct btrfs_item).

       So instead of passing 1 as the minimum data size to push_leaf_right(),
       pass a size that corresponds to the sum of the size of all the
       remaining items in our leaf. We are not interested in moving less than
       that, because if we do, we are not able to delete our leaf and we have
       COWed the right leaf for nothing. Plus, moving only some of the items
       of our leaf, it means an even less balanced tree.

       Just like the previous case, we want to avoid the useless COW of the
       right leaf, this way we don't have to spend time allocating one new
       metadata extent, and doing more IO when committing a transaction or
       syncing a log tree. For the log tree case it's specially more important
       because more IO can result in a higher latency for a fsync operation.

    So adjust the minimum data size passed to push_leaf_left() and
    push_leaf_right() as mentioned above.

    This change if part of a patchset that is comprised of the following
    patches:

      1/6 btrfs: remove unnecessary leaf free space checks when pushing items
      2/6 btrfs: avoid unnecessary COW of leaves when deleting items from a leaf
      3/6 btrfs: avoid unnecessary computation when deleting items from a leaf
      4/6 btrfs: remove constraint on number of visited leaves when replacing extents
      5/6 btrfs: remove useless path release in the fast fsync path
      6/6 btrfs: prepare extents to be logged before locking a log tree path

    Not being able to delete a leaf that became less than 1/3 full after
    deleting items from it is actually common. For example, for the fio test
    mentioned in the changelog of patch 6/6, we are only able to delete a
    leaf at btrfs_del_items() about 5.3% of the time, due to its left and
    right neighbour leaves not having enough free space to push all the
    remaining items into them.

    The last patch in the series has some performance test result in its
    changelog.

    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 3bbc62f546b932707f0492e3622edc017cee82a9
Author: Filipe Manana <fdmanana@suse.com>
Date:   Thu Feb 3 14:55:45 2022 +0000

    btrfs: remove unnecessary leaf free space checks when pushing items

    When trying to push items from a leaf into its left and right neighbours,
    we lock the left or right leaf, check if it has the required minimum free
    space, COW the leaf and then check again if it has the minimum required
    free space. This second check is pointless:

    1) Most and foremost because it's not needed. We have a write lock on the
       leaf and on its parent node, so no one can come in and change either
       the pre-COW or post-COW version of the leaf for the whole duration of
       the push_leaf_left() and push_leaf_right() calls;

    2) The call to btrfs_leaf_free_space() is not trivial, it has a fair
       amount of arithmetic operations and access to fields in the leaf's
       header and items, so it's not very cheap.

    So remove the duplicated free space checks.

    This change if part of a patchset that is comprised of the following
    patches:

      1/6 btrfs: remove unnecessary leaf free space checks when pushing items
      2/6 btrfs: avoid unnecessary COW of leaves when deleting items from a leaf
      3/6 btrfs: avoid unnecessary computation when deleting items from a leaf
      4/6 btrfs: remove constraint on number of visited leaves when replacing extents
      5/6 btrfs: remove useless path release in the fast fsync path
      6/6 btrfs: prepare extents to be logged before locking a log tree path

    The last patch in the series has some performance test result in its
    changelog.

    Signed-off-by: Filipe Manana <fdmanana@suse.com>
    Signed-off-by: David Sterba <dsterba@suse.com>

commit 6ec66007c12eefc4806b6e9ad803c0df7713e836
Author: David Sterba <dsterba@suse.com>
Date:   Tue Feb 1 15:42:07 2022 +0100

    btrfs: replace BUILD_BUG_ON by static_assert

    The static_assert introduced in 6bab69c65013 ("build_bug.h: add wrapper
    for _Static_assert") has been supported by compilers for a long time
    (gcc 4.6, clang 3.0) and can be used in header files. We don't need to
    put BUILD_BUG_ON to random functions but rather keep it next to the
    definition.

    The exception here is the UAPI header btrfs_tree.h that could be
    potentially included by userspace code and the static assert is not
    defined (nor used in any other header).

    Reviewed-by: Johannes Thumshirn <johannes.thumshirn@wdc.com>
    Signed-off-by: David Sterba <dsterba@suse.com>
---
 fs/btrfs/compression.h     |   2 +
 fs/btrfs/ctree.c           |  66 +++++-----
 fs/btrfs/ctree.h           |  38 ++++--
 fs/btrfs/file-item.c       |   1 +
 fs/btrfs/file.c            |  26 ++--
 fs/btrfs/inode.c           |  23 ++--
 fs/btrfs/ioctl.c           | 240 +++++++++++++++++++++++--------------
 fs/btrfs/lzo.c             |  20 +++-
 fs/btrfs/qgroup.c          |  41 +++++--
 fs/btrfs/sysfs.c           |  10 +-
 fs/btrfs/tree-log.c        |  65 +++++-----
 include/uapi/linux/btrfs.h |   6 +-
 12 files changed, 325 insertions(+), 213 deletions(-)

diff --git a/fs/btrfs/compression.h b/fs/btrfs/compression.h
index 56eef0821e3e..7dbd14caab01 100644
--- a/fs/btrfs/compression.h
+++ b/fs/btrfs/compression.h
@@ -22,6 +22,8 @@ struct btrfs_inode;
 
 /* Maximum length of compressed data stored on disk */
 #define BTRFS_MAX_COMPRESSED		(SZ_128K)
+static_assert((BTRFS_MAX_COMPRESSED % PAGE_SIZE) == 0);
+
 /* Maximum size of data before compression */
 #define BTRFS_MAX_UNCOMPRESSED		(SZ_128K)
 
diff --git a/fs/btrfs/ctree.c b/fs/btrfs/ctree.c
index a7db3f6f1b7b..1bb5335c6d75 100644
--- a/fs/btrfs/ctree.c
+++ b/fs/btrfs/ctree.c
@@ -2990,16 +2990,11 @@ static int push_leaf_right(struct btrfs_trans_handle *trans, struct btrfs_root
 	if (free_space < data_size)
 		goto out_unlock;
 
-	/* cow and double check */
 	ret = btrfs_cow_block(trans, root, right, upper,
 			      slot + 1, &right, BTRFS_NESTING_RIGHT_COW);
 	if (ret)
 		goto out_unlock;
 
-	free_space = btrfs_leaf_free_space(right);
-	if (free_space < data_size)
-		goto out_unlock;
-
 	left_nritems = btrfs_header_nritems(left);
 	if (left_nritems == 0)
 		goto out_unlock;
@@ -3224,7 +3219,6 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		goto out;
 	}
 
-	/* cow and double check */
 	ret = btrfs_cow_block(trans, root, left,
 			      path->nodes[1], slot - 1, &left,
 			      BTRFS_NESTING_LEFT_COW);
@@ -3235,12 +3229,6 @@ static int push_leaf_left(struct btrfs_trans_handle *trans, struct btrfs_root
 		goto out;
 	}
 
-	free_space = btrfs_leaf_free_space(left);
-	if (free_space < data_size) {
-		ret = 1;
-		goto out;
-	}
-
 	if (check_sibling_keys(left, right)) {
 		ret = -EUCLEAN;
 		goto out;
@@ -4170,24 +4158,22 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 {
 	struct btrfs_fs_info *fs_info = root->fs_info;
 	struct extent_buffer *leaf;
-	u32 last_off;
-	u32 dsize = 0;
 	int ret = 0;
 	int wret;
-	int i;
 	u32 nritems;
 
 	leaf = path->nodes[0];
-	last_off = btrfs_item_offset(leaf, slot + nr - 1);
-
-	for (i = 0; i < nr; i++)
-		dsize += btrfs_item_size(leaf, slot + i);
-
 	nritems = btrfs_header_nritems(leaf);
 
 	if (slot + nr != nritems) {
-		int data_end = leaf_data_end(leaf);
+		const u32 last_off = btrfs_item_offset(leaf, slot + nr - 1);
+		const int data_end = leaf_data_end(leaf);
 		struct btrfs_map_token token;
+		u32 dsize = 0;
+		int i;
+
+		for (i = 0; i < nr; i++)
+			dsize += btrfs_item_size(leaf, slot + i);
 
 		memmove_extent_buffer(leaf, BTRFS_LEAF_DATA_OFFSET +
 			      data_end + dsize,
@@ -4227,24 +4213,50 @@ int btrfs_del_items(struct btrfs_trans_handle *trans, struct btrfs_root *root,
 			fixup_low_keys(path, &disk_key, 1);
 		}
 
-		/* delete the leaf if it is mostly empty */
+		/*
+		 * Try to delete the leaf if it is mostly empty. We do this by
+		 * trying to move all its items into its left and right neighbours.
+		 * If we can't move all the items, then we don't delete it - it's
+		 * not ideal, but future insertions might fill the leaf with more
+		 * items, or items from other leaves might be moved later into our
+		 * leaf due to deletions on those leaves.
+		 */
 		if (used < BTRFS_LEAF_DATA_SIZE(fs_info) / 3) {
+			u32 min_push_space;
+
 			/* push_leaf_left fixes the path.
 			 * make sure the path still points to our leaf
 			 * for possible call to del_ptr below
 			 */
 			slot = path->slots[1];
 			atomic_inc(&leaf->refs);
-
-			wret = push_leaf_left(trans, root, path, 1, 1,
-					      1, (u32)-1);
+			/*
+			 * We want to be able to at least push one item to the
+			 * left neighbour leaf, and that's the first item.
+			 */
+			min_push_space = sizeof(struct btrfs_item) +
+				btrfs_item_size(leaf, 0);
+			wret = push_leaf_left(trans, root, path, 0,
+					      min_push_space, 1, (u32)-1);
 			if (wret < 0 && wret != -ENOSPC)
 				ret = wret;
 
 			if (path->nodes[0] == leaf &&
 			    btrfs_header_nritems(leaf)) {
-				wret = push_leaf_right(trans, root, path, 1,
-						       1, 1, 0);
+				/*
+				 * If we were not able to push all items from our
+				 * leaf to its left neighbour, then attempt to
+				 * either push all the remaining items to the
+				 * right neighbour or none. There's no advantage
+				 * in pushing only some items, instead of all, as
+				 * it's pointless to end up with a leaf having
+				 * too few items while the neighbours can be full
+				 * or nearly full.
+				 */
+				nritems = btrfs_header_nritems(leaf);
+				min_push_space = leaf_space_used(leaf, 0, nritems);
+				wret = push_leaf_right(trans, root, path, 0,
+						       min_push_space, 1, 0);
 				if (wret < 0 && wret != -ENOSPC)
 					ret = wret;
 			}
diff --git a/fs/btrfs/ctree.h b/fs/btrfs/ctree.h
index 8992e0096163..3f45a905a791 100644
--- a/fs/btrfs/ctree.h
+++ b/fs/btrfs/ctree.h
@@ -1599,25 +1599,25 @@ DECLARE_BTRFS_SETGET_BITS(64)
 static inline u##bits btrfs_##name(const struct extent_buffer *eb,	\
 				   const type *s)			\
 {									\
-	BUILD_BUG_ON(sizeof(u##bits) != sizeof(((type *)0))->member);	\
+	static_assert(sizeof(u##bits) == sizeof(((type *)0))->member);	\
 	return btrfs_get_##bits(eb, s, offsetof(type, member));		\
 }									\
 static inline void btrfs_set_##name(const struct extent_buffer *eb, type *s, \
 				    u##bits val)			\
 {									\
-	BUILD_BUG_ON(sizeof(u##bits) != sizeof(((type *)0))->member);	\
+	static_assert(sizeof(u##bits) == sizeof(((type *)0))->member);	\
 	btrfs_set_##bits(eb, s, offsetof(type, member), val);		\
 }									\
 static inline u##bits btrfs_token_##name(struct btrfs_map_token *token,	\
 					 const type *s)			\
 {									\
-	BUILD_BUG_ON(sizeof(u##bits) != sizeof(((type *)0))->member);	\
+	static_assert(sizeof(u##bits) == sizeof(((type *)0))->member);	\
 	return btrfs_get_token_##bits(token, s, offsetof(type, member));\
 }									\
 static inline void btrfs_set_token_##name(struct btrfs_map_token *token,\
 					  type *s, u##bits val)		\
 {									\
-	BUILD_BUG_ON(sizeof(u##bits) != sizeof(((type *)0))->member);	\
+	static_assert(sizeof(u##bits) == sizeof(((type *)0))->member);	\
 	btrfs_set_token_##bits(token, s, offsetof(type, member), val);	\
 }
 
@@ -1648,8 +1648,8 @@ static inline void btrfs_set_##name(type *s, u##bits val)		\
 static inline u64 btrfs_device_total_bytes(const struct extent_buffer *eb,
 					   struct btrfs_dev_item *s)
 {
-	BUILD_BUG_ON(sizeof(u64) !=
-		     sizeof(((struct btrfs_dev_item *)0))->total_bytes);
+	static_assert(sizeof(u64) ==
+		      sizeof(((struct btrfs_dev_item *)0))->total_bytes);
 	return btrfs_get_64(eb, s, offsetof(struct btrfs_dev_item,
 					    total_bytes));
 }
@@ -1657,8 +1657,8 @@ static inline void btrfs_set_device_total_bytes(const struct extent_buffer *eb,
 						struct btrfs_dev_item *s,
 						u64 val)
 {
-	BUILD_BUG_ON(sizeof(u64) !=
-		     sizeof(((struct btrfs_dev_item *)0))->total_bytes);
+	static_assert(sizeof(u64) ==
+		      sizeof(((struct btrfs_dev_item *)0))->total_bytes);
 	WARN_ON(!IS_ALIGNED(val, eb->fs_info->sectorsize));
 	btrfs_set_64(eb, s, offsetof(struct btrfs_dev_item, total_bytes), val);
 }
@@ -3270,9 +3270,27 @@ int btrfs_fileattr_set(struct user_namespace *mnt_userns,
 int btrfs_ioctl_get_supported_features(void __user *arg);
 void btrfs_sync_inode_flags_to_i_flags(struct inode *inode);
 int __pure btrfs_is_empty_uuid(u8 *uuid);
+
+struct btrfs_defrag_ctrl {
+	/* Input, read-only fields */
+	u64	start;
+	u64	len;
+	u32	extent_thresh;
+	u64	newer_than;
+	u64	max_sectors_to_defrag;
+	u8	compress;
+	u8	flags;
+
+	/* Output fields */
+	u64	sectors_defragged;
+	u64	last_scanned;	/* Exclusive bytenr */
+};
+int btrfs_defrag_ioctl_args_to_ctrl(struct btrfs_fs_info *fs_info,
+				    struct btrfs_ioctl_defrag_range_args *args,
+				    struct btrfs_defrag_ctrl *ctrl,
+				    u64 max_sectors_to_defrag, u64 newer_than);
 int btrfs_defrag_file(struct inode *inode, struct file_ra_state *ra,
-		      struct btrfs_ioctl_defrag_range_args *range,
-		      u64 newer_than, unsigned long max_to_defrag);
+		      struct btrfs_defrag_ctrl *ctrl);
 void btrfs_get_block_group_info(struct list_head *groups_list,
 				struct btrfs_ioctl_space_info *space);
 void btrfs_update_ioctl_balance_args(struct btrfs_fs_info *fs_info,
diff --git a/fs/btrfs/file-item.c b/fs/btrfs/file-item.c
index 90c5c38836ab..9a3de652ada8 100644
--- a/fs/btrfs/file-item.c
+++ b/fs/btrfs/file-item.c
@@ -1211,6 +1211,7 @@ void btrfs_extent_item_to_extent_map(struct btrfs_inode *inode,
 	extent_start = key.offset;
 	extent_end = btrfs_file_extent_end(path);
 	em->ram_bytes = btrfs_file_extent_ram_bytes(leaf, fi);
+	em->generation = btrfs_file_extent_generation(leaf, fi);
 	if (type == BTRFS_FILE_EXTENT_REG ||
 	    type == BTRFS_FILE_EXTENT_PREALLOC) {
 		em->start = extent_start;
diff --git a/fs/btrfs/file.c b/fs/btrfs/file.c
index 11204dbbe053..12e63be6a35b 100644
--- a/fs/btrfs/file.c
+++ b/fs/btrfs/file.c
@@ -277,8 +277,7 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 {
 	struct btrfs_root *inode_root;
 	struct inode *inode;
-	struct btrfs_ioctl_defrag_range_args range;
-	int num_defrag;
+	struct btrfs_defrag_ctrl ctrl = {0};
 	int ret;
 
 	/* get the inode */
@@ -297,21 +296,23 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 
 	/* do a chunk of defrag */
 	clear_bit(BTRFS_INODE_IN_DEFRAG, &BTRFS_I(inode)->runtime_flags);
-	memset(&range, 0, sizeof(range));
-	range.len = (u64)-1;
-	range.start = defrag->last_offset;
+	ctrl.len = (u64)-1;
+	ctrl.start = defrag->last_offset;
+	ctrl.newer_than = defrag->transid;
+	ctrl.max_sectors_to_defrag = BTRFS_DEFRAG_BATCH;
 
 	sb_start_write(fs_info->sb);
-	num_defrag = btrfs_defrag_file(inode, NULL, &range, defrag->transid,
-				       BTRFS_DEFRAG_BATCH);
+	ret = btrfs_defrag_file(inode, NULL, &ctrl);
 	sb_end_write(fs_info->sb);
+	if (ret < 0)
+		goto out;
 	/*
 	 * if we filled the whole defrag batch, there
 	 * must be more work to do.  Queue this defrag
 	 * again
 	 */
-	if (num_defrag == BTRFS_DEFRAG_BATCH) {
-		defrag->last_offset = range.start;
+	if (ctrl.sectors_defragged == BTRFS_DEFRAG_BATCH) {
+		defrag->last_offset = ctrl.last_scanned;
 		btrfs_requeue_inode_defrag(BTRFS_I(inode), defrag);
 	} else if (defrag->last_offset && !defrag->cycled) {
 		/*
@@ -325,7 +326,7 @@ static int __btrfs_run_defrag_inode(struct btrfs_fs_info *fs_info,
 	} else {
 		kmem_cache_free(btrfs_inode_defrag_cachep, defrag);
 	}
-
+out:
 	iput(inode);
 	return 0;
 cleanup:
@@ -718,7 +719,6 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	int modify_tree = -1;
 	int update_refs;
 	int found = 0;
-	int leafs_visited = 0;
 	struct btrfs_path *path = args->path;
 
 	args->bytes_found = 0;
@@ -756,7 +756,6 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				path->slots[0]--;
 		}
 		ret = 0;
-		leafs_visited++;
 next_slot:
 		leaf = path->nodes[0];
 		if (path->slots[0] >= btrfs_header_nritems(leaf)) {
@@ -768,7 +767,6 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 				ret = 0;
 				break;
 			}
-			leafs_visited++;
 			leaf = path->nodes[0];
 			recow = 1;
 		}
@@ -1014,7 +1012,7 @@ int btrfs_drop_extents(struct btrfs_trans_handle *trans,
 	 * which case it unlocked our path, so check path->locks[0] matches a
 	 * write lock.
 	 */
-	if (!ret && args->replace_extent && leafs_visited == 1 &&
+	if (!ret && args->replace_extent &&
 	    path->locks[0] == BTRFS_WRITE_LOCK &&
 	    btrfs_leaf_free_space(leaf) >=
 	    sizeof(struct btrfs_item) + args->extent_item_size) {
diff --git a/fs/btrfs/inode.c b/fs/btrfs/inode.c
index 3b2403b6127f..bd44a0783174 100644
--- a/fs/btrfs/inode.c
+++ b/fs/btrfs/inode.c
@@ -624,7 +624,6 @@ static noinline int compress_file_range(struct async_chunk *async_chunk)
 again:
 	will_compress = 0;
 	nr_pages = (end >> PAGE_SHIFT) - (start >> PAGE_SHIFT) + 1;
-	BUILD_BUG_ON((BTRFS_MAX_COMPRESSED % PAGE_SIZE) != 0);
 	nr_pages = min_t(unsigned long, nr_pages,
 			BTRFS_MAX_COMPRESSED / PAGE_SIZE);
 
@@ -5584,21 +5583,17 @@ static struct inode *new_simple_dir(struct super_block *s,
 	return inode;
 }
 
+static_assert(BTRFS_FT_UNKNOWN == FT_UNKNOWN);
+static_assert(BTRFS_FT_REG_FILE == FT_REG_FILE);
+static_assert(BTRFS_FT_DIR == FT_DIR);
+static_assert(BTRFS_FT_CHRDEV == FT_CHRDEV);
+static_assert(BTRFS_FT_BLKDEV == FT_BLKDEV);
+static_assert(BTRFS_FT_FIFO == FT_FIFO);
+static_assert(BTRFS_FT_SOCK == FT_SOCK);
+static_assert(BTRFS_FT_SYMLINK == FT_SYMLINK);
+
 static inline u8 btrfs_inode_type(struct inode *inode)
 {
-	/*
-	 * Compile-time asserts that generic FT_* types still match
-	 * BTRFS_FT_* types
-	 */
-	BUILD_BUG_ON(BTRFS_FT_UNKNOWN != FT_UNKNOWN);
-	BUILD_BUG_ON(BTRFS_FT_REG_FILE != FT_REG_FILE);
-	BUILD_BUG_ON(BTRFS_FT_DIR != FT_DIR);
-	BUILD_BUG_ON(BTRFS_FT_CHRDEV != FT_CHRDEV);
-	BUILD_BUG_ON(BTRFS_FT_BLKDEV != FT_BLKDEV);
-	BUILD_BUG_ON(BTRFS_FT_FIFO != FT_FIFO);
-	BUILD_BUG_ON(BTRFS_FT_SOCK != FT_SOCK);
-	BUILD_BUG_ON(BTRFS_FT_SYMLINK != FT_SYMLINK);
-
 	return fs_umode_to_ftype(inode->i_mode);
 }
 
diff --git a/fs/btrfs/ioctl.c b/fs/btrfs/ioctl.c
index 927771d1853f..23ceea70a696 100644
--- a/fs/btrfs/ioctl.c
+++ b/fs/btrfs/ioctl.c
@@ -1046,23 +1046,37 @@ static struct extent_map *defrag_lookup_extent(struct inode *inode, u64 start,
 	return em;
 }
 
+static u32 get_extent_max_capacity(const struct extent_map *em)
+{
+	if (test_bit(EXTENT_FLAG_COMPRESSED, &em->flags))
+		return BTRFS_MAX_COMPRESSED;
+	return BTRFS_MAX_EXTENT_SIZE;
+}
+
 static bool defrag_check_next_extent(struct inode *inode, struct extent_map *em,
 				     bool locked)
 {
 	struct extent_map *next;
-	bool ret = true;
+	bool ret = false;
 
 	/* this is the last extent */
 	if (em->start + em->len >= i_size_read(inode))
-		return false;
+		return ret;
 
 	next = defrag_lookup_extent(inode, em->start + em->len, locked);
+	/* No more em or hole */
 	if (!next || next->block_start >= EXTENT_MAP_LAST_BYTE)
-		ret = false;
-	else if ((em->block_start + em->block_len == next->block_start) &&
-		 (em->block_len > SZ_128K && next->block_len > SZ_128K))
-		ret = false;
-
+		goto out;
+	if (test_bit(EXTENT_FLAG_PREALLOC, &next->flags))
+		goto out;
+	/*
+	 * If the next extent is at its max capcity, defragging current extent
+	 * makes no sense, as the total number of extents won't change.
+	 */
+	if (next->len >= get_extent_max_capacity(em))
+		goto out;
+	ret = true;
+out:
 	free_extent_map(next);
 	return ret;
 }
@@ -1172,22 +1186,21 @@ struct defrag_target_range {
 /*
  * Collect all valid target extents.
  *
+ * @ctrl:	   extra defrag policy control
  * @start:	   file offset to lookup
  * @len:	   length to lookup
- * @extent_thresh: file extent size threshold, any extent size >= this value
- *		   will be ignored
- * @newer_than:    only defrag extents newer than this value
- * @do_compress:   whether the defrag is doing compression
- *		   if true, @extent_thresh will be ignored and all regular
- *		   file extents meeting @newer_than will be targets.
  * @locked:	   if the range has already held extent lock
  * @target_list:   list of targets file extents
+ *
+ * Will update ctrl::last_scanned.
  */
 static int defrag_collect_targets(struct btrfs_inode *inode,
-				  u64 start, u64 len, u32 extent_thresh,
-				  u64 newer_than, bool do_compress,
-				  bool locked, struct list_head *target_list)
+				  struct btrfs_defrag_ctrl *ctrl,
+				  u64 start, u32 len, bool locked,
+				  struct list_head *target_list)
 {
+	bool do_compress = ctrl->flags & BTRFS_DEFRAG_RANGE_COMPRESS;
+	bool last_is_target = false;
 	u64 cur = start;
 	int ret = 0;
 
@@ -1197,6 +1210,7 @@ static int defrag_collect_targets(struct btrfs_inode *inode,
 		bool next_mergeable = true;
 		u64 range_len;
 
+		last_is_target = false;
 		em = defrag_lookup_extent(&inode->vfs_inode, cur, locked);
 		if (!em)
 			break;
@@ -1207,7 +1221,7 @@ static int defrag_collect_targets(struct btrfs_inode *inode,
 			goto next;
 
 		/* Skip older extent */
-		if (em->generation < newer_than)
+		if (em->generation < ctrl->newer_than)
 			goto next;
 
 		/* This em is under writeback, no need to defrag */
@@ -1251,7 +1265,14 @@ static int defrag_collect_targets(struct btrfs_inode *inode,
 			goto add;
 
 		/* Skip too large extent */
-		if (range_len >= extent_thresh)
+		if (range_len >= ctrl->extent_thresh)
+			goto next;
+
+		/*
+		 * Skip extents already at its max capacity, this is mostly for
+		 * compressed extents, which max cap is only 128K.
+		 */
+		if (em->len >= get_extent_max_capacity(em))
 			goto next;
 
 		next_mergeable = defrag_check_next_extent(&inode->vfs_inode, em,
@@ -1272,6 +1293,7 @@ static int defrag_collect_targets(struct btrfs_inode *inode,
 		}
 
 add:
+		last_is_target = true;
 		range_len = min(extent_map_end(em), start + len) - cur;
 		/*
 		 * This one is a good target, check if it can be merged into
@@ -1315,10 +1337,27 @@ static int defrag_collect_targets(struct btrfs_inode *inode,
 			kfree(entry);
 		}
 	}
+	if (!ret) {
+		/*
+		 * If the last extent is not a target, the caller can skip to
+		 * the end of that extent.
+		 * Otherwise, we can only go the end of the spcified range.
+		 *
+		 * And we may got a range smaller than current
+		 * ctrl->last_scanned (e.g. executed in the defrag_one_range
+		 * call), so we have to ensure we didn't decrease
+		 * ctrl->last_scanned.
+		 */
+		if (!last_is_target)
+			ctrl->last_scanned = max(cur, ctrl->last_scanned);
+		else
+			ctrl->last_scanned = max(start + len, ctrl->last_scanned);
+	}
 	return ret;
 }
 
 #define CLUSTER_SIZE	(SZ_256K)
+static_assert(IS_ALIGNED(CLUSTER_SIZE, PAGE_SIZE));
 
 /*
  * Defrag one contiguous target range.
@@ -1372,8 +1411,8 @@ static int defrag_one_locked_target(struct btrfs_inode *inode,
 	return ret;
 }
 
-static int defrag_one_range(struct btrfs_inode *inode, u64 start, u32 len,
-			    u32 extent_thresh, u64 newer_than, bool do_compress)
+static int defrag_one_range(struct btrfs_inode *inode,
+			    struct btrfs_defrag_ctrl *ctrl, u64 start, u32 len)
 {
 	struct extent_state *cached_state = NULL;
 	struct defrag_target_range *entry;
@@ -1417,8 +1456,7 @@ static int defrag_one_range(struct btrfs_inode *inode, u64 start, u32 len,
 	 * And this time we have extent locked already, pass @locked = true
 	 * so that we won't relock the extent range and cause deadlock.
 	 */
-	ret = defrag_collect_targets(inode, start, len, extent_thresh,
-				     newer_than, do_compress, true,
+	ret = defrag_collect_targets(inode, ctrl, start, len, true,
 				     &target_list);
 	if (ret < 0)
 		goto unlock_extent;
@@ -1428,6 +1466,8 @@ static int defrag_one_range(struct btrfs_inode *inode, u64 start, u32 len,
 					       &cached_state);
 		if (ret < 0)
 			break;
+		ctrl->sectors_defragged += entry->len >>
+					   inode->root->fs_info->sectorsize_bits;
 	}
 
 	list_for_each_entry_safe(entry, tmp, &target_list, list) {
@@ -1449,12 +1489,17 @@ static int defrag_one_range(struct btrfs_inode *inode, u64 start, u32 len,
 	return ret;
 }
 
+/*
+ * Return <0 for error.
+ * Return >0 if we hit the ctrl->max_sectors_to_defrag limit
+ * Return 0 if we finished the range without error.
+ *
+ * For >= 0 case, ctrl->last_scanned and ctrl->sectors_defragged will be updated.
+ */
 static int defrag_one_cluster(struct btrfs_inode *inode,
 			      struct file_ra_state *ra,
-			      u64 start, u32 len, u32 extent_thresh,
-			      u64 newer_than, bool do_compress,
-			      unsigned long *sectors_defragged,
-			      unsigned long max_sectors)
+			      struct btrfs_defrag_ctrl *ctrl,
+			      u64 start, u32 len)
 {
 	const u32 sectorsize = inode->root->fs_info->sectorsize;
 	struct defrag_target_range *entry;
@@ -1462,9 +1507,7 @@ static int defrag_one_cluster(struct btrfs_inode *inode,
 	LIST_HEAD(target_list);
 	int ret;
 
-	BUILD_BUG_ON(!IS_ALIGNED(CLUSTER_SIZE, PAGE_SIZE));
-	ret = defrag_collect_targets(inode, start, len, extent_thresh,
-				     newer_than, do_compress, false,
+	ret = defrag_collect_targets(inode, ctrl, start, len, false,
 				     &target_list);
 	if (ret < 0)
 		goto out;
@@ -1473,32 +1516,25 @@ static int defrag_one_cluster(struct btrfs_inode *inode,
 		u32 range_len = entry->len;
 
 		/* Reached or beyond the limit */
-		if (max_sectors && *sectors_defragged >= max_sectors) {
+		if (ctrl->max_sectors_to_defrag &&
+		    ctrl->sectors_defragged >= ctrl->max_sectors_to_defrag) {
 			ret = 1;
 			break;
 		}
 
-		if (max_sectors)
+		if (ctrl->max_sectors_to_defrag)
 			range_len = min_t(u32, range_len,
-				(max_sectors - *sectors_defragged) * sectorsize);
+				(ctrl->max_sectors_to_defrag -
+				 ctrl->sectors_defragged) * sectorsize);
 
 		if (ra)
 			page_cache_sync_readahead(inode->vfs_inode.i_mapping,
 				ra, NULL, entry->start >> PAGE_SHIFT,
 				((entry->start + range_len - 1) >> PAGE_SHIFT) -
 				(entry->start >> PAGE_SHIFT) + 1);
-		/*
-		 * Here we may not defrag any range if holes are punched before
-		 * we locked the pages.
-		 * But that's fine, it only affects the @sectors_defragged
-		 * accounting.
-		 */
-		ret = defrag_one_range(inode, entry->start, range_len,
-				       extent_thresh, newer_than, do_compress);
+		ret = defrag_one_range(inode, ctrl, entry->start, range_len);
 		if (ret < 0)
 			break;
-		*sectors_defragged += range_len >>
-				      inode->root->fs_info->sectorsize_bits;
 	}
 out:
 	list_for_each_entry_safe(entry, tmp, &target_list, list) {
@@ -1508,64 +1544,93 @@ static int defrag_one_cluster(struct btrfs_inode *inode,
 	return ret;
 }
 
+/*
+ * Convert the old ioctl format to the new btrfs_defrag_ctrl structure.
+ *
+ * Will also do basic tasks like setting default values and sanity checks.
+ */
+int btrfs_defrag_ioctl_args_to_ctrl(struct btrfs_fs_info *fs_info,
+				    struct btrfs_ioctl_defrag_range_args *args,
+				    struct btrfs_defrag_ctrl *ctrl,
+				    u64 max_sectors_to_defrag, u64 newer_than)
+{
+	u64 range_end;
+
+	if (args->flags & ~BTRFS_DEFRAG_RANGE_FLAGS_MASK)
+		return -EOPNOTSUPP;
+	if (args->compress_type >= BTRFS_NR_COMPRESS_TYPES)
+		return -EOPNOTSUPP;
+
+	ctrl->start = round_down(args->start, fs_info->sectorsize);
+	/*
+	 * If @len does not overflow with @start nor is -1, align the length.
+	 * Otherwise set it to (u64)-1 so later btrfs_defrag_file() will
+	 * determine the length using isize.
+	 */
+	if (!check_add_overflow(args->start, args->len, &range_end) &&
+	    args->len != (u64)-1)
+		ctrl->len = round_up(range_end, fs_info->sectorsize) -
+			    ctrl->start;
+	else
+		ctrl->len = -1;
+	ctrl->flags = args->flags;
+	ctrl->compress = args->compress_type;
+	if (args->extent_thresh == 0)
+		ctrl->extent_thresh = SZ_256K;
+	else
+		ctrl->extent_thresh = args->extent_thresh;
+	ctrl->newer_than = newer_than;
+	ctrl->last_scanned = 0;
+	ctrl->sectors_defragged = 0;
+	return 0;
+}
+
 /*
  * Entry point to file defragmentation.
  *
  * @inode:	   inode to be defragged
  * @ra:		   readahead state (can be NUL)
- * @range:	   defrag options including range and flags
- * @newer_than:	   minimum transid to defrag
- * @max_to_defrag: max number of sectors to be defragged, if 0, the whole inode
- *		   will be defragged.
+ * @ctrl:	   defrag options including range and various policy parameters
  *
  * Return <0 for error.
- * Return >=0 for the number of sectors defragged, and range->start will be updated
- * to indicate the file offset where next defrag should be started at.
- * (Mostly for autodefrag, which sets @max_to_defrag thus we may exit early without
- *  defragging all the range).
+ * Return 0 if the defrag is done without error, ctrl->last_scanned and
+ * ctrl->sectors_defragged will be updated.
  */
 int btrfs_defrag_file(struct inode *inode, struct file_ra_state *ra,
-		      struct btrfs_ioctl_defrag_range_args *range,
-		      u64 newer_than, unsigned long max_to_defrag)
+		      struct btrfs_defrag_ctrl *ctrl)
 {
 	struct btrfs_fs_info *fs_info = btrfs_sb(inode->i_sb);
-	unsigned long sectors_defragged = 0;
 	u64 isize = i_size_read(inode);
 	u64 cur;
 	u64 last_byte;
-	bool do_compress = range->flags & BTRFS_DEFRAG_RANGE_COMPRESS;
+	bool do_compress = ctrl->flags & BTRFS_DEFRAG_RANGE_COMPRESS;
 	bool ra_allocated = false;
-	int compress_type = BTRFS_COMPRESS_ZLIB;
 	int ret = 0;
-	u32 extent_thresh = range->extent_thresh;
 	pgoff_t start_index;
 
 	if (isize == 0)
 		return 0;
 
-	if (range->start >= isize)
+	if (ctrl->start >= isize)
 		return -EINVAL;
 
-	if (do_compress) {
-		if (range->compress_type >= BTRFS_NR_COMPRESS_TYPES)
-			return -EINVAL;
-		if (range->compress_type)
-			compress_type = range->compress_type;
-	}
+	if (do_compress)
+		ASSERT(ctrl->compress < BTRFS_NR_COMPRESS_TYPES);
 
-	if (extent_thresh == 0)
-		extent_thresh = SZ_256K;
+	if (ctrl->extent_thresh == 0)
+		ctrl->extent_thresh = SZ_256K;
 
-	if (range->start + range->len > range->start) {
+	if (ctrl->start + ctrl->len > ctrl->start) {
 		/* Got a specific range */
-		last_byte = min(isize, range->start + range->len);
+		last_byte = min(isize, ctrl->start + ctrl->len);
 	} else {
 		/* Defrag until file end */
 		last_byte = isize;
 	}
 
 	/* Align the range */
-	cur = round_down(range->start, fs_info->sectorsize);
+	cur = round_down(ctrl->start, fs_info->sectorsize);
+	ctrl->last_scanned = cur;
 	last_byte = round_up(last_byte, fs_info->sectorsize) - 1;
 
 	/*
@@ -1589,12 +1654,9 @@ int btrfs_defrag_file(struct inode *inode, struct file_ra_state *ra,
 		inode->i_mapping->writeback_index = start_index;
 
 	while (cur < last_byte) {
-		const unsigned long prev_sectors_defragged = sectors_defragged;
+		const unsigned long prev_sectors_defragged = ctrl->sectors_defragged;
 		u64 cluster_end;
 
-		/* The cluster size 256K should always be page aligned */
-		BUILD_BUG_ON(!IS_ALIGNED(CLUSTER_SIZE, PAGE_SIZE));
-
 		if (btrfs_defrag_cancelled(fs_info)) {
 			ret = -EAGAIN;
 			break;
@@ -1616,19 +1678,17 @@ int btrfs_defrag_file(struct inode *inode, struct file_ra_state *ra,
 			break;
 		}
 		if (do_compress)
-			BTRFS_I(inode)->defrag_compress = compress_type;
-		ret = defrag_one_cluster(BTRFS_I(inode), ra, cur,
-				cluster_end + 1 - cur, extent_thresh,
-				newer_than, do_compress,
-				&sectors_defragged, max_to_defrag);
+			BTRFS_I(inode)->defrag_compress = ctrl->compress;
+		ret = defrag_one_cluster(BTRFS_I(inode), ra, ctrl, cur,
+				cluster_end + 1 - cur);
 
-		if (sectors_defragged > prev_sectors_defragged)
+		if (ctrl->sectors_defragged > prev_sectors_defragged)
 			balance_dirty_pages_ratelimited(inode->i_mapping);
 
 		btrfs_inode_unlock(inode, 0);
 		if (ret < 0)
 			break;
-		cur = cluster_end + 1;
+		cur = max(cluster_end + 1, ctrl->last_scanned);
 		if (ret > 0) {
 			ret = 0;
 			break;
@@ -1638,27 +1698,21 @@ int btrfs_defrag_file(struct inode *inode, struct file_ra_state *ra,
 
 	if (ra_allocated)
 		kfree(ra);
-	/*
-	 * Update range.start for autodefrag, this will indicate where to start
-	 * in next run.
-	 */
-	range->start = cur;
-	if (sectors_defragged) {
+	if (ctrl->sectors_defragged) {
 		/*
 		 * We have defragged some sectors, for compression case they
 		 * need to be written back immediately.
 		 */
-		if (range->flags & BTRFS_DEFRAG_RANGE_START_IO) {
+		if (ctrl->flags & BTRFS_DEFRAG_RANGE_START_IO) {
 			filemap_flush(inode->i_mapping);
 			if (test_bit(BTRFS_INODE_HAS_ASYNC_EXTENT,
 				     &BTRFS_I(inode)->runtime_flags))
 				filemap_flush(inode->i_mapping);
 		}
-		if (range->compress_type == BTRFS_COMPRESS_LZO)
+		if (ctrl->compress == BTRFS_COMPRESS_LZO)
 			btrfs_set_fs_incompat(fs_info, COMPRESS_LZO);
-		else if (range->compress_type == BTRFS_COMPRESS_ZSTD)
+		else if (ctrl->compress == BTRFS_COMPRESS_ZSTD)
 			btrfs_set_fs_incompat(fs_info, COMPRESS_ZSTD);
-		ret = sectors_defragged;
 	}
 	if (do_compress) {
 		btrfs_inode_lock(inode, 0);
@@ -3179,6 +3233,7 @@ static int btrfs_ioctl_defrag(struct file *file, void __user *argp)
 	struct inode *inode = file_inode(file);
 	struct btrfs_root *root = BTRFS_I(inode)->root;
 	struct btrfs_ioctl_defrag_range_args range = {0};
+	struct btrfs_defrag_ctrl ctrl = {0};
 	int ret;
 
 	ret = mnt_want_write_file(file);
@@ -3224,8 +3279,11 @@ static int btrfs_ioctl_defrag(struct file *file, void __user *argp)
 			/* the rest are all set to zero by kzalloc */
 			range.len = (u64)-1;
 		}
-		ret = btrfs_defrag_file(file_inode(file), &file->f_ra,
-					&range, BTRFS_OLDEST_GENERATION, 0);
+		ret = btrfs_defrag_ioctl_args_to_ctrl(root->fs_info, &range,
+						      &ctrl, 0, BTRFS_OLDEST_GENERATION);
+		if (ret < 0)
+			break;
+		ret = btrfs_defrag_file(file_inode(file), &file->f_ra, &ctrl);
 		if (ret > 0)
 			ret = 0;
 		break;
diff --git a/fs/btrfs/lzo.c b/fs/btrfs/lzo.c
index 0fb90cbe7669..430ad36b8b08 100644
--- a/fs/btrfs/lzo.c
+++ b/fs/btrfs/lzo.c
@@ -55,6 +55,9 @@
  * 0x1000   | SegHdr N+1| Data payload N+1 ...                |
  */
 
+#define WORKSPACE_BUF_LENGTH	(lzo1x_worst_compress(PAGE_SIZE))
+#define WORKSPACE_CBUF_LENGTH	(lzo1x_worst_compress(PAGE_SIZE))
+
 struct workspace {
 	void *mem;
 	void *buf;	/* where decompressed data goes */
@@ -83,8 +86,8 @@ struct list_head *lzo_alloc_workspace(unsigned int level)
 		return ERR_PTR(-ENOMEM);
 
 	workspace->mem = kvmalloc(LZO1X_MEM_COMPRESS, GFP_KERNEL);
-	workspace->buf = kvmalloc(lzo1x_worst_compress(PAGE_SIZE), GFP_KERNEL);
-	workspace->cbuf = kvmalloc(lzo1x_worst_compress(PAGE_SIZE), GFP_KERNEL);
+	workspace->buf = kvmalloc(WORKSPACE_BUF_LENGTH, GFP_KERNEL);
+	workspace->cbuf = kvmalloc(WORKSPACE_CBUF_LENGTH, GFP_KERNEL);
 	if (!workspace->mem || !workspace->buf || !workspace->cbuf)
 		goto fail;
 
@@ -380,6 +383,17 @@ int lzo_decompress_bio(struct list_head *ws, struct compressed_bio *cb)
 		kunmap(cur_page);
 		cur_in += LZO_LEN;
 
+		if (seg_len > WORKSPACE_CBUF_LENGTH) {
+			/*
+			 * seg_len shouldn't be larger than we have allocated
+			 * for workspace->cbuf
+			 */
+			btrfs_err(fs_info, "unexpectedly large lzo segment len %u",
+					seg_len);
+			ret = -EIO;
+			goto out;
+		}
+
 		/* Copy the compressed segment payload into workspace */
 		copy_compressed_segment(cb, workspace->cbuf, seg_len, &cur_in);
 
@@ -422,7 +436,7 @@ int lzo_decompress(struct list_head *ws, unsigned char *data_in,
 	struct workspace *workspace = list_entry(ws, struct workspace, list);
 	size_t in_len;
 	size_t out_len;
-	size_t max_segment_len = lzo1x_worst_compress(PAGE_SIZE);
+	size_t max_segment_len = WORKSPACE_BUF_LENGTH;
 	int ret = 0;
 	char *kaddr;
 	unsigned long bytes;
diff --git a/fs/btrfs/qgroup.c b/fs/btrfs/qgroup.c
index f12dc687350c..b43f3eb62bb9 100644
--- a/fs/btrfs/qgroup.c
+++ b/fs/btrfs/qgroup.c
@@ -258,16 +258,19 @@ static int del_qgroup_rb(struct btrfs_fs_info *fs_info, u64 qgroupid)
 	return 0;
 }
 
-/* must be called with qgroup_lock held */
-static int add_relation_rb(struct btrfs_fs_info *fs_info,
-			   u64 memberid, u64 parentid)
+/*
+ * Add relation specified by two qgroups.
+ *
+ * Must be called with qgroup_lock held.
+ *
+ * Return: 0        on success
+ *         -ENOENT  if one of the qgroups is NULL
+ *         <0       other errors
+ */
+static int __add_relation_rb(struct btrfs_qgroup *member, struct btrfs_qgroup *parent)
 {
-	struct btrfs_qgroup *member;
-	struct btrfs_qgroup *parent;
 	struct btrfs_qgroup_list *list;
 
-	member = find_qgroup_rb(fs_info, memberid);
-	parent = find_qgroup_rb(fs_info, parentid);
 	if (!member || !parent)
 		return -ENOENT;
 
@@ -283,7 +286,27 @@ static int add_relation_rb(struct btrfs_fs_info *fs_info,
 	return 0;
 }
 
-/* must be called with qgroup_lock held */
+/*
+ * Add relation specified by two qgoup ids.
+ *
+ * Must be called with qgroup_lock held.
+ *
+ * Return: 0        on success
+ *         -ENOENT  if one of the ids does not exist
+ *         <0       other errors
+ */
+static int add_relation_rb(struct btrfs_fs_info *fs_info, u64 memberid, u64 parentid)
+{
+	struct btrfs_qgroup *member;
+	struct btrfs_qgroup *parent;
+
+	member = find_qgroup_rb(fs_info, memberid);
+	parent = find_qgroup_rb(fs_info, parentid);
+
+	return __add_relation_rb(member, parent);
+}
+
+/* Must be called with qgroup_lock held */
 static int del_relation_rb(struct btrfs_fs_info *fs_info,
 			   u64 memberid, u64 parentid)
 {
@@ -1444,7 +1467,7 @@ int btrfs_add_qgroup_relation(struct btrfs_trans_handle *trans, u64 src,
 	}
 
 	spin_lock(&fs_info->qgroup_lock);
-	ret = add_relation_rb(fs_info, src, dst);
+	ret = __add_relation_rb(member, parent);
 	if (ret < 0) {
 		spin_unlock(&fs_info->qgroup_lock);
 		goto out;
diff --git a/fs/btrfs/sysfs.c b/fs/btrfs/sysfs.c
index beb7f72d50b8..5701a565d96b 100644
--- a/fs/btrfs/sysfs.c
+++ b/fs/btrfs/sysfs.c
@@ -1104,6 +1104,11 @@ static inline struct btrfs_fs_info *to_fs_info(struct kobject *kobj)
 static char btrfs_unknown_feature_names[FEAT_MAX][NUM_FEATURE_BITS][BTRFS_FEATURE_NAME_MAX];
 static struct btrfs_feature_attr btrfs_feature_attrs[FEAT_MAX][NUM_FEATURE_BITS];
 
+static_assert(ARRAY_SIZE(btrfs_unknown_feature_names) ==
+	      ARRAY_SIZE(btrfs_feature_attrs));
+static_assert(ARRAY_SIZE(btrfs_unknown_feature_names[0]) ==
+	      ARRAY_SIZE(btrfs_feature_attrs[0]));
+
 static const u64 supported_feature_masks[FEAT_MAX] = {
 	[FEAT_COMPAT]    = BTRFS_FEATURE_COMPAT_SUPP,
 	[FEAT_COMPAT_RO] = BTRFS_FEATURE_COMPAT_RO_SUPP,
@@ -1272,11 +1277,6 @@ static void init_feature_attrs(void)
 	struct btrfs_feature_attr *fa;
 	int set, i;
 
-	BUILD_BUG_ON(ARRAY_SIZE(btrfs_unknown_feature_names) !=
-		     ARRAY_SIZE(btrfs_feature_attrs));
-	BUILD_BUG_ON(ARRAY_SIZE(btrfs_unknown_feature_names[0]) !=
-		     ARRAY_SIZE(btrfs_feature_attrs[0]));
-
 	memset(btrfs_feature_attrs, 0, sizeof(btrfs_feature_attrs));
 	memset(btrfs_unknown_feature_names, 0,
 	       sizeof(btrfs_unknown_feature_names));
diff --git a/fs/btrfs/tree-log.c b/fs/btrfs/tree-log.c
index 3ee014c06b82..6eb7bdc945a6 100644
--- a/fs/btrfs/tree-log.c
+++ b/fs/btrfs/tree-log.c
@@ -4550,14 +4550,34 @@ static int log_one_extent(struct btrfs_trans_handle *trans,
 {
 	struct btrfs_drop_extents_args drop_args = { 0 };
 	struct btrfs_root *log = inode->root->log_root;
-	struct btrfs_file_extent_item *fi;
+	struct btrfs_file_extent_item fi = { 0 };
 	struct extent_buffer *leaf;
-	struct btrfs_map_token token;
 	struct btrfs_key key;
 	u64 extent_offset = em->start - em->orig_start;
 	u64 block_len;
 	int ret;
 
+	btrfs_set_stack_file_extent_generation(&fi, trans->transid);
+	if (test_bit(EXTENT_FLAG_PREALLOC, &em->flags))
+		btrfs_set_stack_file_extent_type(&fi, BTRFS_FILE_EXTENT_PREALLOC);
+	else
+		btrfs_set_stack_file_extent_type(&fi, BTRFS_FILE_EXTENT_REG);
+
+	block_len = max(em->block_len, em->orig_block_len);
+	if (em->compress_type != BTRFS_COMPRESS_NONE) {
+		btrfs_set_stack_file_extent_disk_bytenr(&fi, em->block_start);
+		btrfs_set_stack_file_extent_disk_num_bytes(&fi, block_len);
+	} else if (em->block_start < EXTENT_MAP_LAST_BYTE) {
+		btrfs_set_stack_file_extent_disk_bytenr(&fi, em->block_start -
+							extent_offset);
+		btrfs_set_stack_file_extent_disk_num_bytes(&fi, block_len);
+	}
+
+	btrfs_set_stack_file_extent_offset(&fi, extent_offset);
+	btrfs_set_stack_file_extent_num_bytes(&fi, em->len);
+	btrfs_set_stack_file_extent_ram_bytes(&fi, em->ram_bytes);
+	btrfs_set_stack_file_extent_compression(&fi, em->compress_type);
+
 	ret = log_extent_csums(trans, inode, log, em, ctx);
 	if (ret)
 		return ret;
@@ -4576,7 +4596,7 @@ static int log_one_extent(struct btrfs_trans_handle *trans,
 		drop_args.start = em->start;
 		drop_args.end = em->start + em->len;
 		drop_args.replace_extent = true;
-		drop_args.extent_item_size = sizeof(*fi);
+		drop_args.extent_item_size = sizeof(fi);
 		ret = btrfs_drop_extents(trans, log, inode, &drop_args);
 		if (ret)
 			return ret;
@@ -4588,44 +4608,14 @@ static int log_one_extent(struct btrfs_trans_handle *trans,
 		key.offset = em->start;
 
 		ret = btrfs_insert_empty_item(trans, log, path, &key,
-					      sizeof(*fi));
+					      sizeof(fi));
 		if (ret)
 			return ret;
 	}
 	leaf = path->nodes[0];
-	btrfs_init_map_token(&token, leaf);
-	fi = btrfs_item_ptr(leaf, path->slots[0],
-			    struct btrfs_file_extent_item);
-
-	btrfs_set_token_file_extent_generation(&token, fi, trans->transid);
-	if (test_bit(EXTENT_FLAG_PREALLOC, &em->flags))
-		btrfs_set_token_file_extent_type(&token, fi,
-						 BTRFS_FILE_EXTENT_PREALLOC);
-	else
-		btrfs_set_token_file_extent_type(&token, fi,
-						 BTRFS_FILE_EXTENT_REG);
-
-	block_len = max(em->block_len, em->orig_block_len);
-	if (em->compress_type != BTRFS_COMPRESS_NONE) {
-		btrfs_set_token_file_extent_disk_bytenr(&token, fi,
-							em->block_start);
-		btrfs_set_token_file_extent_disk_num_bytes(&token, fi, block_len);
-	} else if (em->block_start < EXTENT_MAP_LAST_BYTE) {
-		btrfs_set_token_file_extent_disk_bytenr(&token, fi,
-							em->block_start -
-							extent_offset);
-		btrfs_set_token_file_extent_disk_num_bytes(&token, fi, block_len);
-	} else {
-		btrfs_set_token_file_extent_disk_bytenr(&token, fi, 0);
-		btrfs_set_token_file_extent_disk_num_bytes(&token, fi, 0);
-	}
-
-	btrfs_set_token_file_extent_offset(&token, fi, extent_offset);
-	btrfs_set_token_file_extent_num_bytes(&token, fi, em->len);
-	btrfs_set_token_file_extent_ram_bytes(&token, fi, em->ram_bytes);
-	btrfs_set_token_file_extent_compression(&token, fi, em->compress_type);
-	btrfs_set_token_file_extent_encryption(&token, fi, 0);
-	btrfs_set_token_file_extent_other_encoding(&token, fi, 0);
+	write_extent_buffer(leaf, &fi,
+			    btrfs_item_ptr_offset(leaf, path->slots[0]),
+			    sizeof(fi));
 	btrfs_mark_buffer_dirty(leaf);
 
 	btrfs_release_path(path);
@@ -4839,7 +4829,6 @@ static int btrfs_log_changed_extents(struct btrfs_trans_handle *trans,
 	WARN_ON(!list_empty(&extents));
 	write_unlock(&tree->lock);
 
-	btrfs_release_path(path);
 	if (!ret)
 		ret = btrfs_log_prealloc_extents(trans, inode, path);
 	if (ret)
diff --git a/include/uapi/linux/btrfs.h b/include/uapi/linux/btrfs.h
index 738619994e26..012a71ab5d8e 100644
--- a/include/uapi/linux/btrfs.h
+++ b/include/uapi/linux/btrfs.h
@@ -575,8 +575,10 @@ struct btrfs_ioctl_clone_range_args {
  * Used by:
  * struct btrfs_ioctl_defrag_range_args.flags
  */
-#define BTRFS_DEFRAG_RANGE_COMPRESS 1
-#define BTRFS_DEFRAG_RANGE_START_IO 2
+#define BTRFS_DEFRAG_RANGE_COMPRESS	(1UL << 0)
+#define BTRFS_DEFRAG_RANGE_START_IO	(1UL << 1)
+#define BTRFS_DEFRAG_RANGE_FLAGS_MASK	(BTRFS_DEFRAG_RANGE_COMPRESS |\
+					 BTRFS_DEFRAG_RANGE_START_IO)
 struct btrfs_ioctl_defrag_range_args {
 	/* start of the defrag operation */
 	__u64 start;
-- 
2.35.1

